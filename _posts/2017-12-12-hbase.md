---
layout: post
published: true
title: HBase 的架构及设计
---
# HBase 的架构及设计

hbase是强一致性的海量数据库，无论是读写性能，或是数据容量，还是一致性方面，hbase都有非常优秀的表现。

本文从架构方面探讨hbase的主要设计，从而在需要hbase的场合能够更好的设计和判断。

首先，先来看看hbase的整体架构。除了DFS组件，hbase的基本组件图实际上就是Zookeeper，HMaster，RegionServer

其中，RegionServer作为数据的实际存取服务器，主要负责数据的最终存取，一般情况都是多台；

RegionServer根据不同的row key划分为许多region，每个region按顺序存放从startKey到endKey的数据。每个RegionServer有下面这些组件：

一个WAL: write ahead log. 听名知其意，该文件是落库前先写的日志文件，它最主要的作用是恢复数据用，类似于mysql的binlog。保存在HDFS中。
一个BlockCache: regionServer的读缓存。保存使用最频繁的数据，使用LRU算法换出不需要的数据。
多个Region: 每个region包含多个store，每个CF拥有一个store。
store: 每个store包含多个storeFile和一个memstore。
Memstore: region的写缓存。保存还未写入HFile的数据，写入数据前会先做排序，每个region每个CF都会拥有一个Memstore，这就是为什么CF不能建太多的原因。
storeFile: 真正存储keyvalue数据的文件，其保存的文件是排序过的。一个storeFile对应一个HFile。保存在HDFS中。

HFile分为数据块，索引块，bloom过滤器以及trailer。



Trailer主要记录了HFile的基本信息，各个部分的偏移和地址信息。
Data block主要存储用户的key-value数据。
Bloom filter主要用来快速定位文件是否不在数据块。



比较容易混淆的是zookeeper和hmaster。
Zookeeper负责保持多台Hmaster中只有一台是活跃的；存储Hbase的schema，table，CF等元信息；存储所有的region入口；监控regionServer的状态，并将该信息通知hmaster。可以看出来，zookeeper几乎是负责整个集群的关键信息存取以及关键状态监控。如果zookeeper挂了，那么整个hbase集群几乎就是不可用的状态。



Hmaster则是负责对table元数据的管理；对HRegion的负载均衡，调整HRegion的布局，比如分裂和合并；包括恢复数据的迁移等。Hmaster相当于对RegionServer的后台管理，对于一些定制的管理行为，zookeeper不可能帮你完成，于是乎才有了hmaster。如果hmaster挂了，除了不能对table进行管理配置，不能扩展region，并不会影响整体服务的可用性。

接下来我们来关注一些关键流程。

## 客户端首次读写的流程：

1. 客户端首先从zookeeper中得到META table的位置，根据META table的存储位置得到具体的RegionServer是哪台
2. 询问具体的RegionServer

## 写流程：

1. 首先写入WAL日志，以防crash。
2. 紧接着写入Memstore，即写缓存。由于是内存写入，速度较快。
3. 立马返回客户端表示写入完毕。
4. 当Memstore满时，从Memstore刷新到HFile，磁盘的顺序写速度非常快，并记录下最后一次最高的sequence号。这样系统能知道哪些记录已经持久化，哪些没有。

## 读流程：

1. 首先到读缓存BlockCache中查找可能被缓存的数据
2. 如果未找到，到写缓存查找已提交但是未落HFile的数据
3. 如果还未找到， 到HFile中继续查找数据



## 数据紧凑：

数据从memStore刷新到HFile时，为了保持简单，都是每个memStore放一个HFile，这会带来大量小HFile文件，使得查询时效率相对较低，于是，采用数据紧凑的方式将多个小文件压缩为几个大文件。其中，minor compaction是自动将相关的小文件做一些适当的紧凑，但不彻底；而major compaction则是放在午夜跑的定时任务，将文件做最大化的紧凑。



## 数据恢复流程：

当RegionServer挂了，zookeeper很快就能检测到，于是将其下的region状态设置为不可用。Hmaster随即开始恢复的流程。
1. HFile本身有2个备份，而且有专门的HDFS来管理其下的文件。因此对HFile来说并不需要恢复。
2. Hmaster重置region到新的regionServer
3. 之前在MemStore中丢失的数据，通过WAL分裂先将WAL按照region切分。切分的原因是WAL并不区分region，而是所有region的log都写入同一个WAL。
4. 根据WAL回放并恢复数据。回放的过程实际上先进MemStore，再flush到HFile


